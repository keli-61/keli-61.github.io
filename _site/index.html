

<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Ke Li (李珂) | Principal Researcher and Team Manager @Tencent</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Ke Li (李珂)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Principal Researcher and Team Manager @Tencent" />
<meta property="og:description" content="Principal Researcher and Team Manager @Tencent" />
<link rel="canonical" href="http://0.0.0.0:4000/" />
<meta property="og:url" content="http://0.0.0.0:4000/" />
<meta property="og:site_name" content="Ke Li (李珂)" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Ke Li (李珂)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Principal Researcher and Team Manager @Tencent","headline":"Ke Li (李珂)","name":"Ke Li (李珂)","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://0.0.0.0:4000/assets/img/DCIM.jpeg"}},"url":"http://0.0.0.0:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/colors-auto.css?v=">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-45M6MBPF31"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-45M6MBPF31');
  </script>



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://0.0.0.0:4000/">Ke Li (李珂)</a></h1>

        
          <img src="/assets/img/DCIM.jpeg" alt="Logo" />
        

        <p>Principal Researcher and Team Manager @Tencent</p>

        

        
        <ul class="downloads">
          <li><a href="mailto:tristanli.sh@gmail.com">Contact <strong>Email</strong></a></li>
          <li><a href="https://scholar.google.com/citations?user=mfWsFM0AAAAJ&hl=en">Google <strong>Scholar</strong></a></li>
          <li><a href="https://github.com/ieted">View On <strong>GitHub</strong></a></li>
        </ul>
        
      </header>
      <section>

      <h2 id="bio">Bio</h2>
<p>I am a Principal Researcher and Team Manager at <a href="https://open.youtu.qq.com/#/open">Tencent Youtu Lab</a>. My research interests lie in the area of deep learning and its application in computer vision and natural language processing. Before joining Tencent, I recerived my M.S. degree from Xiamen University in 2018 under the supervision of <a href="https://mac.xmu.edu.cn/rrji_en/"><em>Prof. Rongrong Ji</em></a>. I received my B.S. degree from Zhengzhou University in 2015 under the supervision of <a href="https://scholar.google.com.hk/citations?user=u-8x34cAAAAJ&amp;hl=zh-CN"><em>Prof. Mingliang Xu</em></a>.</p>

<h2 id="activities">Activities</h2>

<ul>
  <li>Reviewer for ICML, ICLR, NeurIPS, CVPR, ICCV, ECCV, AAAI and TPAMI.</li>
</ul>

<h2 id="selected-publications">Selected Publications</h2>

<p>Below are some of the works that represent my main research interests. Full paper list (including preprints) could be found at <a href="https://scholar.google.com/citations?user=mfWsFM0AAAAJ&amp;hl=en">Google Scholar</a>.</p>

<p>(* corresponding author)</p>

<table>
  <tbody>
    <tr>
      <td><strong>PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</strong><br />Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, <strong>Ke Li</strong>, Peng Zhai, Lihua Zhang.<br />Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024.<br /><a href="https://arxiv.org/abs/2405.19266">Paper</a>, <a href="https://github.com/ydk122024/PediatricsGPT">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ydk122024/PediatricsGPT.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Align before Collaborate: Mitigating Feature Misalignment for Robust Multi-Agent Perception</strong><br />Kun Yang, Dingkang Yang, <strong>Ke Li</strong>, Dongling Xiao, Zedian Shao, Peng Sun, Liang Song.<br />European Conference on Computer Vision (<strong>ECCV, Oral</strong>), 2024<br /><a href="https://keli.info">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Towards Multimodal Sentiment Analysis Debiasing via Bias Purification</strong><br />Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu, Kun Yang, Zhaoyu Chen, Yuzheng Wang, Peng Zhai, <strong>Ke Li</strong>, Lihua Zhang.<br />European Conference on Computer Vision (<strong>ECCV</strong>), 2024<br /><a href="https://arxiv.org/abs/2403.05023">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Integrating Global Context Contrast and Local Sensitivity for Blind Image Quality Assessment</strong><br />XuDong Li, Runze Hu, Jingyuan Zheng, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, <strong>Ke Li</strong>, Yunhang Shen, Yutao Liu, Pingyang Dai, Rongrong Ji.<br />International Conference on Machine Learning (<strong>ICML, spotlight</strong>), 2024<br /><a href="https://openreview.net/pdf?id=MRYS3Zb4iV">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Adaptive Feature Selection for No-Reference Image Quality Assessment by Mitigating Semantic Noise Sensitivity</strong><br />Xudong Li, Timin Gao, Runze Hu, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Jingyuan Zheng, Yunhang Shen, <strong>Ke Li</strong>, Yutao Liu, Pingyang Dai, Rongrong Ji.<br />International Conference on Machine Learning (<strong>ICML</strong>), 2024<br /><a href="https://arxiv.org/abs/2312.06158">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Training-free Transformer Architecture Search with Zero-cost Proxy Guided Evolution</strong><br />Qinqin Zhou, Kekai Sheng, Xiawu Zheng, <strong>Ke Li</strong>, Yonghong Tian, Jie Chen, Rongrong Ji.<br />IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024.<br /><a href="https://ieeexplore.ieee.org/document/10475573">Paper</a>, <a href="https://github.com/decemberzhou/TF_TAS">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/decemberzhou/TF_TAS.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Sinkhorn Distance Minimization for Knowledge Distillation</strong><br />Xiao Cui, Yulei Qin, Yuting Gao, Enwei Zhang, Zihan Xu, Tong Wu, <strong>Ke Li</strong>, Xing Sun, Wengang Zhou, Houqiang Li.<br />International Conference on Computational Linguistics (<strong>COLING</strong>), 2024<br /><a href="https://arxiv.org/abs/2402.17110">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Aligning and Prompting Everything All at Once for Universal Visual Perception</strong><br />Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, <strong>Ke Li</strong>, Xing Sun, Yunsheng Wu, Shaohui Lin, Rongrong Ji.<br />Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024<br /><a href="https://arxiv.org/abs/2312.02153">Paper</a>, <a href="https://github.com/shenyunhang/APE">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/shenyunhang/APE.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>A General and Efficient Training for Transformer via Token Expansion</strong><br />Wenxuan Huang, Yunhang Shen, Jiao Xie, Baochang Zhang, Gaoqi He, <strong>Ke Li</strong>, Xing Sun, Shaohui Lin.<br />Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024<br /><a href="https://keli.info/">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Solving the Catastrophic Forgetting Problem in Generalized Category Discovery</strong><br />Xinzi Cao, Xiawu Zheng, Guanhong Wang, Weijiang Yu, Yunhang Shen, <strong>Ke Li</strong>, Yutong Lu, Yonghong Tian.<br />Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024<br /><a href="https://keli.info/">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental Learning</strong><br />Xialei Liu, Jiang-Tian Zhai, Andrew D. Bagdanov, <strong>Ke Li</strong>, Ming-Ming Cheng.<br />Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024<br /><a href="https://keli.info/">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Weakly Supervised Open-Vocabulary Object Detection</strong><br />Jianghang Lin, Yunhang Shen, Bingquan Wang, Shaohui Lin, <strong>Ke Li</strong>, Liujuan Cao.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024<br /><a href="https://arxiv.org/abs/2312.12437">Paper</a></td>
    </tr>
    <tr>
      <td><strong>SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric Positive Definite Space</strong><br />Yunchen Li, Zhou Yu, Gaoqi He, Yunhang Shen, <strong>Ke Li</strong>, Xing Sun, Shaohui Lin.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024<br /><a href="https://arxiv.org/abs/2312.08200">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Semi-Supervised Blind Image Quality Assessment through Knowledge Distillation and Incremental Learning</strong><br />Wensheng Pan, Timin Gao, Yan Zhang, Xiawu Zheng, Yunhang Shen, <strong>Ke Li</strong>, Runze Hu, Yutao Liu, Pingyang Dai.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024<br /><a href="https://keli.info/">Paper</a></td>
    </tr>
    <tr>
      <td><strong>SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger</strong><br />Yuting Gao, Jinfeng Liu, Zihan Xu, Tong Wu, Enwei Zhang, <strong>Ke Li</strong>, Jie Yang, Wei Liu, Xing Sun.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024<br /><a href="https://arxiv.org/abs/2303.17561">Paper</a></td>
    </tr>
    <tr>
      <td><strong>CAPro: Webly Supervised Learning with Cross-modality Aligned Prototypes</strong><br />Yulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou Fu, Yun Gu, <strong>Ke Li</strong>, Xing Sun, Rongrong Ji.<br />Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023.<br /><a href="http://keli.info">Paper</a>, <a href="https://github.com/yuleiqin/capro">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/yuleiqin/capro.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Multi-modal Queried Object Detection in the Wild</strong><br />Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen, Xiaoshan Yang, <strong>Ke Li</strong>, Changsheng Xu.<br />Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023.<br /><a href="https://arxiv.org/abs/2305.18980">Paper</a>, <a href="https://github.com/YifanXu74/MQ-Det">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/YifanXu74/MQ-Det.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>LocLoc: Low-level Cues and Local-area Guides for Weakly Supervised Object Localization</strong><br />Xinzi Cao, Xiawu Zheng, Yunhang Shen, <strong>Ke Li</strong>, Jie Chen, Yutong Lu, Yonghong Tian.<br />ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2023.<br /><a href="http://keli.info">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Masked Autoencoders are Efficient Class Incremental Learners</strong><br />Jiang-Tian Zhai, Xialei Liu, Andy Bagdanov, <strong>Ke Li</strong>, Ming-Ming Cheng.<br />International Conference on Computer Vision (<strong>ICCV</strong>), 2023.<br /><a href="https://arxiv.org/abs/2308.12510">Paper</a>, <a href="https://github.com/scok30/MAE-CIL">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/scok30/MAE-CIL.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Woodpecker: Hallucination Correction for Multimodal Large Language Models</strong><br />Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, <strong>Ke Li</strong>, Xing Sun, Enhong Chen.<br />arxiv, 2023<br /><a href="https://arxiv.org/abs/2310.16045">Paper</a>, <a href="https://github.com/BradyFU/Woodpecker">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/BradyFU/Woodpecker.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</strong><br />Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, <strong>Ke Li*</strong>, Xing Sun, Rongrong Ji.<br />arxiv, 2023<br /><a href="https://arxiv.org/abs/2306.13394">Paper</a>, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>A Survey on Multimodal Large Language Models</strong><br />Shukang Yin , Chaoyou Fu, Sirui Zhao, <strong>Ke Li</strong>, Xing Sun, Tong Xu, Enhong Chen.<br />arxiv, 2023<br /><a href="https://arxiv.org/abs/2306.13549">Paper</a>, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>CF-ViT: A General Coarse-to-Fine Method for Vision Transformer</strong><br />Mengzhao Chen, Mingbao Lin, <strong>Ke Li</strong>, Yunhang Shen, Yongjian Wu, Fei Chao, Rongrong Ji.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI, Oral</strong>), 2023<br /><a href="https://arxiv.org/abs/2203.03821">Paper</a>, <a href="https://github.com/ChenMnZ/CF-ViT">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ChenMnZ/CF-ViT.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Adaptive Hierarchy-Branch Fusion for Online Knowledge Distillation</strong><br />Linrui Gong, Shaohui Lin, Baochang Zhang, Yunhang Shen, <strong>Ke Li</strong>, Ruizhi Qiao, Bo Ren, Muqing Li, Zhou Yu, Lizhuang Ma.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2023<br /><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25937">Paper</a></td>
    </tr>
    <tr>
      <td><strong>PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining</strong><br />Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, <strong>Ke Li</strong>, Rongrong Ji, Chunhua Shen.<br />Advances in Neural Information Processing Systems (<strong>NeurIPS, Oral</strong>), 2022<br /><a href="https://arxiv.org/abs/2204.14095">Paper</a>, <a href="https://github.com/Yuting-Gao/PyramidCLIP">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Yuting-Gao/PyramidCLIP.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Learning Best Combination for Efficient N:M Sparsity</strong><br />Yuxin Zhang, Mingbao Lin, Zhihang Lin, Yiting Luo, <strong>Ke Li</strong>, Fei Chao, Yongjian Wu, Rongrong Ji.<br />Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022<br /><a href="https://arxiv.org/abs/2206.06662">Paper</a>, <a href="https://github.com/zyxxmu/LBC">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/zyxxmu/LBC.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Fine-grained Data Distribution Alignment for Post-Training Quantization</strong><br />Yunshan Zhong, Mingbao Lin, Mengzhao Chen, <strong>Ke Li</strong>, Yunhang Shen, Fei Chao, Yongjian Wu, Rongrong Ji.<br />European Conference on Computer Vision (<strong>ECCV</strong>), 2022<br /><a href="https://arxiv.org/abs/2109.04186">Paper</a>, <a href="https://github.com/zysxmu/FDDA">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/zysxmu/FDDA.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks</strong><br />Yunshan Zhong, Mingbao Lin, Xunchao Li, <strong>Ke Li</strong>, Yunhang Shen, Fei Chao, Yongjian Wu, Rongrong Ji.<br />European Conference on Computer Vision (<strong>ECCV</strong>), 2022<br /><a href="https://arxiv.org/abs/2203.03844">Paper</a>, <a href="https://github.com/zysxmu/DDTB">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/zysxmu/DDTB.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Long-Tailed Class Incremental Learning</strong><br />Xialei Liu, Yusong Hu, Xu-Sheng Cao, Andy Bagdanov, <strong>Ke Li</strong>, Ming-Ming Cheng.<br />European Conference on Computer Vision (<strong>ECCV</strong>), 2022<br /><a href="https://arxiv.org/abs/2210.00266">Paper</a>, <a href="https://github.com/xialeiliu/Long-Tailed-CIL">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/xialeiliu/Long-Tailed-CIL.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>DisCo: Remedying Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning</strong><br />Yuting Gao, Jia-Xin Zhuang, Shaohui Lin, Hao Cheng, Xing Sun, <strong>Ke Li*</strong>, Chunhua Shen.<br />European Conference on Computer Vision (<strong>ECCV, Oral</strong>), 2022<br /><a href="https://arxiv.org/abs/2104.09124">Paper</a>, <a href="https://github.com/Yuting-Gao/DisCo-pytorch">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Yuting-Gao/DisCo-pytorch.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Efficient Decoder-free Object Detection with Transformers</strong><br />Peixian Chen, Mengdan Zhang, Yunhang Shen, Kekai Sheng, Yuting Gao, Xing Sun, <strong>Ke Li*</strong>, Chunhua Shen.<br />European Conference on Computer Vision (<strong>ECCV</strong>), 2022<br /><a href="https://arxiv.org/abs/2206.06829">Paper</a>, <a href="https://github.com/Pealing/DFFT">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Pealing/DFFT.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>ARM: Any-Time Super-Resolution Method</strong><br />Bohong Chen, Mingbao Lin, Kekai Sheng, Mengdan Zhang, Peixian Chen, <strong>Ke Li</strong>, Liujuan Cao, Rongrong Ji.<br />European Conference on Computer Vision (<strong>ECCV</strong>), 2022<br /><a href="https://arxiv.org/abs/2203.10812">Paper</a>, <a href="https://github.com/chenbong/ARM-Net">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/chenbong/ARM-Net.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Self-supervised Models are Good Teaching Assistants for Vision Transformers</strong><br />Haiyan Wu, Yuting Gao, Yinqi Zhang, Shaohui Lin, Yuan Xie, Xing Sun, <strong>Ke Li</strong> .<br />International Conference on Machine Learning (<strong>ICML</strong>), 2022<br /><a href="https://proceedings.mlr.press/v162/wu22c.html">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Training-free Transformer Architecture Search</strong><br />Qinqin Zhou, Kekai Sheng, Xiawu Zheng, <strong>Ke Li</strong>, Xing Sun, Yonghong Tian, Jie Chen, Rongrong Ji .<br />Computer Vision and Pattern Recognition (<strong>CVPR, Oral</strong>), 2022<br /><a href="https://arxiv.org/abs/2203.12217">Paper</a>, <a href="https://github.com/decemberzhou/TF_TAS">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/decemberzhou/TF_TAS.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer</strong><br />Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, <strong>Ke Li</strong>, Weiming Dong, Liqing Zhang, Changsheng Xu, Xing Sun.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022<br /><a href="https://arxiv.org/abs/2108.01390">Paper</a>, <a href="https://github.com/YifanXu74/Evo-ViT">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/YifanXu74/Evo-ViT.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning</strong><br />Jinpeng Wang, Yuting Gao, <strong>Ke Li</strong>, Yiqi Lin, Andy J Ma, Xing Sun.<br />Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021<br /><a href="https://arxiv.org/abs/2009.05769">Paper</a>, <a href="https://github.com/FingerRec/BE">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/FingerRec/BE.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Architecture Disentanglement for Deep Neural Networks</strong><br />Jie Hu, Liujuan Cao, Qixiang Ye, Tong Tong, ShengChuan Zhang, <strong>Ke Li</strong>, Feiyue Huang, Rongrong Ji, Ling Shao.<br />International Conference on Computer Vision (<strong>ICCV, Oral</strong>), 2021<br /><a href="https://arxiv.org/abs/2003.13268">Paper</a>, <a href="https://github.com/hujiecpp/NAD">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/hujiecpp/NAD.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion</strong><br />Jinpeng Wang, Yuting Gao, <strong>Ke Li</strong>, Xinyang Jiang, Xiaowei Guo, Rongrong Ji, Xing Sun.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2021<br /><a href="https://arxiv.org/abs/2009.05757">Paper</a>, <a href="https://github.com/FingerRec/DSM-decoupling-scene-motion">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/FingerRec/DSM-decoupling-scene-motion.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>One for More: Selecting Generalizable Samples for Generalizable ReID Model</strong><br />Enwei Zhang, Xinyang Jiang, Hao Cheng, Ancong Wu, Fufu Yu, <strong>Ke Li</strong>, Xiaowei Guo, Feng Zheng, Wei-Shi Zheng, Xing Sun.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2021<br /><a href="https://arxiv.org/abs/2012.05475">Paper</a></td>
    </tr>
    <tr>
      <td><strong>Pruning Filter in Filter</strong><br />Fanxu Meng, Hao Cheng, <strong>Ke Li</strong>, Huixiang Luo, Xiaowei Guo, Guangming Lu, Xing Sun.<br />Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2020<br /><a href="https://arxiv.org/abs/2009.14410">Paper</a>, <a href="https://github.com/fxmeng/Pruning-Filter-in-Filter">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/fxmeng/Pruning-Filter-in-Filter.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Filter Grafting for Deep Neural Networks</strong><br />Fanxu Meng, Hao Cheng, <strong>Ke Li</strong>, Zhixin Xu, Rongrong Ji, Xing Sun, Gaungming Lu.<br />Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020<br /><a href="https://arxiv.org/abs/2001.05868">Paper</a>, <a href="https://github.com/fxmeng/filter-grafting">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/fxmeng/filter-grafting.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification</strong><br />Fengxiang Yang, <strong>Ke Li</strong>, Zhun Zhong, Zhiming Luo, Xing Sun, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, Shaozi Li.<br />Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020.<br /><a href="https://arxiv.org/abs/1912.01349">Paper</a>, <a href="https://github.com/FlyingRoastDuck/ACT_AAAI20">Code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/FlyingRoastDuck/ACT_AAAI20.svg" alt="GitHub stars" title="" /></td>
    </tr>
    <tr>
      <td><strong>Semi-Supervised Adversarial Monocular Depth Estimation</strong><br />Rongrong Ji, <strong>Ke Li*</strong>, Yan Wang, Feng Guo, Xiaowei Guo, Yongjian Wu, Feiyue Huang, and Jiebo Luo.<br />IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2019.<br /><a href="https://arxiv.org/abs/1908.02126">Paper</a></td>
    </tr>
  </tbody>
</table>

<h2 id="more-about">More About</h2>

<ul>
  <li>Born in Zhengzhou. Now live in Shanghai.</li>
  <li>Aside from the academic experience, I also hold a B.A. degree in English with TEM-8 passed and had been teaching writing skills in the <a href="http://www.neworiental.org/english/">New Oriental</a> part-time during 2011 to 2014.</li>
</ul>



      </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
